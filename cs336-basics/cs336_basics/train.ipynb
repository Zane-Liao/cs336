{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Large Language Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data import get_batch, load, save\n",
    "from tokenizer import Tokenizer, train_bpe, PAT_GPT2, PAT_SPECIAL_TOKEN\n",
    "from modules.layers import TransformerLM\n",
    "from modules.loss import CrossEntropyLoss\n",
    "from modules.activation import GLU, Softmax\n",
    "from modules.optimizer import SGD, AdamW, compute_lr, gradient_cliping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Configuration ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Configuration ---\")\n",
    "with open('config.yaml', 'r') as f:\n",
    "    load = yaml.safe_load(f)\n",
    "        \n",
    "model_args = load['model_args']\n",
    "training_args = load['training_args']\n",
    "data_args = load['data_args']\n",
    "    \n",
    "os.makedirs(data_args['checkpoint_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing ---\n",
      "Using device: cpu\n",
      "Tokenizer loaded. Vocab size: 10000\n",
      "Model created with 17,576,448 parameters.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Initializing ---\")\n",
    "device = torch.device(training_args['device'] if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=data_args['vocab_path'],\n",
    "    merges_filepath=data_args['merges_path']\n",
    "    )\n",
    "\n",
    "model_args['vocab_size'] = len(tokenizer.vocab)\n",
    "print(f\"Tokenizer loaded. Vocab size: {model_args['vocab_size']}\")\n",
    "    \n",
    "model = torch.compile(TransformerLM(**model_args).to(device))\n",
    "print(f\"Model created with {model.get_num_params():,} parameters.\")\n",
    "    \n",
    "optimizer = AdamW(model.parameters(), lr=training_args['learning_rate'])\n",
    "    \n",
    "loss_init = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data with np.memmap ---\n",
      "Train data tokens: 530,486,325, Val data tokens: 5,357,233\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Data with np.memmap ---\")\n",
    "train_data = np.memmap(data_args['train_data_path'], dtype=np.uint16, mode='r')\n",
    "valid_data = np.memmap(data_args['valid_data_path'], dtype=np.uint16, mode='r')\n",
    "print(f\"Train data tokens: {len(train_data):,}, Val data tokens: {len(valid_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_iter = 0\n",
    "if data_args['resume_from_checkpoint']:\n",
    "    print(f\"Resuming training from {data_args['resume_from_checkpoint']}\")\n",
    "    start_iter = load(data_args['resume_from_checkpoint'], model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    eval_iters = 100\n",
    "    for _ in range(eval_iters):\n",
    "        x, y = get_batch(valid_data, training_args['batch_size'], model_args['context_length'], device)\n",
    "        logits = model(x)\n",
    "        loss = loss_init(logits.view(-1, model_args['vocab_size']), y.view(-1))\n",
    "        valid_loss += loss.item()\n",
    "    model.train()\n",
    "    return valid_loss / eval_iters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Begin Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoints/model_iter_1800.pt iterations: 1800\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from data import load\n",
    "\n",
    "ckpts = glob.glob(os.path.join(data_args['checkpoint_dir'], \"model_iter_*.pt\"))\n",
    "if ckpts:\n",
    "    latest_ckpt = max(ckpts, key=os.path.getctime)\n",
    "    start_iter = load(latest_ckpt, model, optimizer) + 1\n",
    "else:\n",
    "    print(\"From Scratch Training\")\n",
    "    start_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training Loop ---\n",
      "Iter 1810/5000, Train Loss: 2.0309, LR: 0.000227, GradNorm: 0.4491, Time: 43163.2ms\n",
      "Iter 1820/5000, Train Loss: 1.9783, LR: 0.000226, GradNorm: 0.4672, Time: 44995.2ms\n",
      "Iter 1830/5000, Train Loss: 1.9211, LR: 0.000225, GradNorm: 0.4525, Time: 43732.2ms\n",
      "Iter 1840/5000, Train Loss: 1.9868, LR: 0.000224, GradNorm: 0.4544, Time: 43826.7ms\n",
      "Iter 1850/5000, Train Loss: 2.0341, LR: 0.000224, GradNorm: 0.4633, Time: 41222.0ms\n",
      "Iter 1860/5000, Train Loss: 1.9187, LR: 0.000223, GradNorm: 0.4503, Time: 41311.7ms\n",
      "Iter 1870/5000, Train Loss: 1.8880, LR: 0.000222, GradNorm: 0.4511, Time: 49969.2ms\n",
      "Iter 1880/5000, Train Loss: 1.9195, LR: 0.000221, GradNorm: 0.4569, Time: 51677.9ms\n",
      "Iter 1890/5000, Train Loss: 1.9518, LR: 0.000220, GradNorm: 0.4443, Time: 41408.6ms\n",
      "Iter 1900/5000, Train Loss: 2.0176, LR: 0.000220, GradNorm: 0.4444, Time: 41096.3ms\n",
      "Iter 1910/5000, Train Loss: 1.9700, LR: 0.000219, GradNorm: 0.4469, Time: 38377.6ms\n",
      "Iter 1920/5000, Train Loss: 1.9611, LR: 0.000218, GradNorm: 0.4322, Time: 41063.7ms\n",
      "Iter 1930/5000, Train Loss: 1.9855, LR: 0.000217, GradNorm: 0.4589, Time: 46188.1ms\n",
      "Iter 1940/5000, Train Loss: 1.9633, LR: 0.000216, GradNorm: 0.4486, Time: 51230.9ms\n",
      "Iter 1950/5000, Train Loss: 1.9501, LR: 0.000216, GradNorm: 0.4470, Time: 45106.7ms\n",
      "Iter 1960/5000, Train Loss: 1.9414, LR: 0.000215, GradNorm: 0.4438, Time: 41723.3ms\n",
      "Iter 1970/5000, Train Loss: 1.8859, LR: 0.000214, GradNorm: 0.4453, Time: 45000.7ms\n",
      "Iter 1980/5000, Train Loss: 1.9601, LR: 0.000213, GradNorm: 0.4698, Time: 46799.9ms\n",
      "Iter 1990/5000, Train Loss: 1.9789, LR: 0.000212, GradNorm: 0.4651, Time: 42894.1ms\n",
      "Iter 2000/5000, Train Loss: 1.8514, LR: 0.000212, GradNorm: 0.4540, Time: 43509.0ms\n",
      "--- Eval at iter 2000: Val Loss: 1.9349 ---\n",
      "save checkpoints/model_iter_2000.pt iterations: 2000\n",
      "Iter 2010/5000, Train Loss: 2.0165, LR: 0.000211, GradNorm: 0.4662, Time: 186521.6ms\n",
      "Iter 2020/5000, Train Loss: 1.9668, LR: 0.000210, GradNorm: 0.4505, Time: 42983.3ms\n",
      "Iter 2030/5000, Train Loss: 1.9870, LR: 0.000209, GradNorm: 0.4481, Time: 39550.6ms\n",
      "Iter 2040/5000, Train Loss: 1.8716, LR: 0.000208, GradNorm: 0.4742, Time: 40976.6ms\n",
      "Iter 2050/5000, Train Loss: 1.9772, LR: 0.000208, GradNorm: 0.4975, Time: 39286.9ms\n",
      "Iter 2060/5000, Train Loss: 1.8888, LR: 0.000207, GradNorm: 0.4408, Time: 44730.6ms\n",
      "Iter 2070/5000, Train Loss: 1.9556, LR: 0.000206, GradNorm: 0.4537, Time: 38001.9ms\n",
      "Iter 2080/5000, Train Loss: 1.9361, LR: 0.000205, GradNorm: 0.4731, Time: 40103.1ms\n",
      "Iter 2090/5000, Train Loss: 1.8847, LR: 0.000204, GradNorm: 0.4538, Time: 35969.8ms\n",
      "Iter 2100/5000, Train Loss: 1.9249, LR: 0.000203, GradNorm: 0.4319, Time: 32578.1ms\n",
      "Iter 2110/5000, Train Loss: 1.9383, LR: 0.000203, GradNorm: 0.4700, Time: 34499.7ms\n",
      "Iter 2120/5000, Train Loss: 2.0265, LR: 0.000202, GradNorm: 0.6039, Time: 34837.3ms\n",
      "Iter 2130/5000, Train Loss: 1.8987, LR: 0.000201, GradNorm: 0.4442, Time: 35219.7ms\n",
      "Iter 2140/5000, Train Loss: 1.9319, LR: 0.000200, GradNorm: 0.4491, Time: 35702.1ms\n",
      "Iter 2150/5000, Train Loss: 1.9043, LR: 0.000199, GradNorm: 0.4512, Time: 39083.8ms\n",
      "Iter 2160/5000, Train Loss: 1.8767, LR: 0.000198, GradNorm: 0.4502, Time: 41011.1ms\n",
      "Iter 2170/5000, Train Loss: 1.9484, LR: 0.000198, GradNorm: 0.4571, Time: 36549.4ms\n",
      "Iter 2180/5000, Train Loss: 1.8349, LR: 0.000197, GradNorm: 0.4744, Time: 36297.3ms\n",
      "Iter 2190/5000, Train Loss: 1.9043, LR: 0.000196, GradNorm: 0.4484, Time: 37198.3ms\n",
      "Iter 2200/5000, Train Loss: 1.9230, LR: 0.000195, GradNorm: 0.4794, Time: 38676.4ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     67\u001b[39m     t0 = t1\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m iter_num > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m iter_num % training_args[\u001b[33m'\u001b[39m\u001b[33meval_interval\u001b[39m\u001b[33m'\u001b[39m] == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     val_loss = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     val_losses.append(val_loss)\n\u001b[32m     72\u001b[39m     eval_iterations.append(iter_num)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[32m      7\u001b[39m     x, y = get_batch(valid_data, training_args[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m], model_args[\u001b[33m'\u001b[39m\u001b[33mcontext_length\u001b[39m\u001b[33m'\u001b[39m], device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     loss = loss_init(logits.view(-\u001b[32m1\u001b[39m, model_args[\u001b[33m'\u001b[39m\u001b[33mvocab_size\u001b[39m\u001b[33m'\u001b[39m]), y.view(-\u001b[32m1\u001b[39m))\n\u001b[32m     10\u001b[39m     valid_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Quant/Artificial Intelligence/cs336/assignment1-basics/cs336_basics/modules/layers.py:436\u001b[39m, in \u001b[36mTransformerLM.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    433\u001b[39m x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_final(x)\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lm_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Quant/Artificial Intelligence/cs336/assignment1-basics/cs336_basics/modules/layers.py:370\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    365\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.self_attn(\u001b[38;5;28mself\u001b[39m.rms_norm1(x),\n\u001b[32m    366\u001b[39m                                  token_positions=token_positions\n\u001b[32m    367\u001b[39m                                 )\n\u001b[32m    368\u001b[39m y = x + attn_output\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrms_norm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Quant/Artificial Intelligence/cs336/assignment1-basics/cs336_basics/modules/layers.py:164\u001b[39m, in \u001b[36mSwiGLU.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.w2(silu(\u001b[38;5;28mself\u001b[39m.w1(x)) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mw3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.13/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/llm_run\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "iterations = []\n",
    "eval_iterations = []\n",
    "\n",
    "print(\"--- Starting Training Loop ---\")\n",
    "t0 = time.time()\n",
    "\n",
    "for iter_num in range(start_iter, training_args['max_iters']):\n",
    "\n",
    "    lr = compute_lr(\n",
    "        iter_num,\n",
    "        training_args['learning_rate'],\n",
    "        training_args['min_lr'],\n",
    "        training_args['warmup_steps'],\n",
    "        training_args['lr_decay_steps']\n",
    "    )\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg['lr'] = lr\n",
    "\n",
    "\n",
    "    inputs, targets = get_batch(\n",
    "        train_data,\n",
    "        training_args['batch_size'],\n",
    "        model_args['context_length'],\n",
    "        device\n",
    "    )\n",
    "    logits = model(inputs)\n",
    "    loss = loss_init(\n",
    "        logits.view(-1, model_args['vocab_size']),\n",
    "        targets.view(-1)\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "        model.parameters(),\n",
    "        training_args['gradient_clip_val']\n",
    "    )\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    iterations.append(iter_num)\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), iter_num)\n",
    "    writer.add_scalar(\"LR\", lr, iter_num)\n",
    "    writer.add_scalar(\"GradNorm/total\", total_norm, iter_num)\n",
    "\n",
    "    if iter_num % 10 == 0:\n",
    "        t1 = time.time()\n",
    "        print(\n",
    "            f\"Iter {iter_num}/{training_args['max_iters']}, \"\n",
    "            f\"Train Loss: {loss.item():.4f}, \"\n",
    "            f\"LR: {lr:.6f}, \"\n",
    "            f\"GradNorm: {total_norm:.4f}, \"\n",
    "            f\"Time: {(t1-t0)*1000:.1f}ms\"\n",
    "        )\n",
    "        t0 = t1\n",
    "\n",
    "    if iter_num > 0 and iter_num % training_args['eval_interval'] == 0:\n",
    "        val_loss = evaluate()\n",
    "        val_losses.append(val_loss)\n",
    "        eval_iterations.append(iter_num)\n",
    "\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, iter_num)\n",
    "\n",
    "        print(f\"--- Eval at iter {iter_num}: Val Loss: {val_loss:.4f} ---\")\n",
    "        checkpoint_path = os.path.join(\n",
    "            data_args['checkpoint_dir'],\n",
    "            f\"model_iter_{iter_num}.pt\"\n",
    "        )\n",
    "        save(model, optimizer, iter_num, checkpoint_path)\n",
    "\n",
    "writer.close()\n",
    "print(\"Training complete! Logs written to runs/llm_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Finished! ---\n",
      "save checkpoints/model_final.pt iterations: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Training Finished! ---\")\n",
    "final_checkpoint_path = os.path.join(data_args['checkpoint_dir'], \"model_final.pt\")\n",
    "save(model, optimizer, training_args['max_iters'], final_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
