{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Large Language Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data import get_batch, load, save\n",
    "from tokenizer import Tokenizer, train_bpe, PAT_GPT2, PAT_SPECIAL_TOKEN\n",
    "from modules.layers import TransformerLM\n",
    "from modules.loss import CrossEntropyLoss\n",
    "from modules.activation import GLU, Softmax\n",
    "from modules.optimizer import SGD, AdamW, compute_lr, gradient_cliping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we need to segment the words. We run the bye-pair encoding algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_corpus_path = \"../../data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "\n",
    "vocab_size = 1268\n",
    "\n",
    "output_dir = 'vocab'\n",
    "output_voacb_path = os.path.join(output_dir, 'vocab_train.json')\n",
    "output_merges_path = os.path.join(output_dir, 'merges.txt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Begin...\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    vocab, merges = train_bpe(\n",
    "        input_path=input_corpus_path,\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=PAT_SPECIAL_TOKEN\n",
    "    )\n",
    "    \n",
    "    print(\"Finish\")\n",
    "    \n",
    "    decoded_vocab = { k: v.decode('utf-8', errors='replace') for k, v in vocab.items() }\n",
    "    with open(output_voacb_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(decoded_vocab, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    print(\"merges...\")\n",
    "    with open(output_merges_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"#BPE Merges\\n\")\n",
    "        for byte_1, byte_2 in merges:\n",
    "            token1_str = byte_1.decode('utf-8', errors='replace')\n",
    "            token2_str = byte_2.decode('utf-8', errors='replace')\n",
    "            f.write(f\"{token1_str} {token2_str}\\n\")\n",
    "\n",
    "    print(\"Finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we preprocess the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_save(input_txt_path, output_bin_path, tokenizer):\n",
    "    with open(input_txt_path, 'r', encoding='utf-8') as f:\n",
    "        text_data = f.read()\n",
    "\n",
    "    tokens = tokenizer.encode(text_data)\n",
    "    \n",
    "    tokens_np = np.array(tokens, dtype=np.uint16)\n",
    "\n",
    "    tokens_np.tofile(output_bin_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vocab_path = 'tokenizer/vocab.json'\n",
    "    merges_path = 'tokenizer/merges.txt'\n",
    "    \n",
    "    input_train_txt_path = 'data/train.txt'\n",
    "    input_val_txt_path = 'data/val.txt'\n",
    "\n",
    "    output_train_bin_path = 'data/train.bin'\n",
    "    output_val_bin_path = 'data/val.bin'\n",
    "\n",
    "    print(\"Begin...\")\n",
    "\n",
    "    tokenizer = Tokenizer.from_files(\n",
    "        vocab_filepath=vocab_path,\n",
    "        merges_filepath=merges_path\n",
    "    )\n",
    "    \n",
    "    tokenize_and_save(input_train_txt_path, output_train_bin_path, tokenizer)\n",
    "    \n",
    "    tokenize_and_save(input_val_txt_path, output_val_bin_path, tokenizer)\n",
    "    \n",
    "    print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now comes the most important part, training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Configuration ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Configuration ---\")\n",
    "with open('config.yaml', 'r') as f:\n",
    "    load = yaml.safe_load(f)\n",
    "        \n",
    "model_args = load['model_args']\n",
    "training_args = load['training_args']\n",
    "data_args = load['data_args']\n",
    "    \n",
    "os.makedirs(data_args['checkpoint_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Initializing ---\")\n",
    "device = torch.device(training_args['device'] if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=data_args['vocab_path'],\n",
    "    merges_filepath=data_args['merges_path']\n",
    "    )\n",
    "\n",
    "model_args['vocab_size'] = len(tokenizer.vocab)\n",
    "print(f\"Tokenizer loaded. Vocab size: {model_args['vocab_size']}\")\n",
    "    \n",
    "model = TransformerLM(**model_args).to(device)\n",
    "print(f\"Model created with {model.get_num_params():,} parameters.\")\n",
    "    \n",
    "optimizer = AdamW(model.parameters(), lr=training_args['learning_rate'])\n",
    "    \n",
    "loss_init = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Loading Data with np.memmap ---\")\n",
    "train_data = np.memmap(data_args['train_data_path'], dtype=np.uint16, model='r')\n",
    "valid_data = np.memmap(data_args['valid_data_path'], dtype=np.uint16, model='r')\n",
    "print(f\"Train data tokens: {len(train_data):,}, Val data tokens: {len(valid_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_iter = 0\n",
    "if data_args['resume_from_checkpoint']:\n",
    "    print(f\"Resuming training from {data_args['resume_from_checkpoint']}\")\n",
    "    start_iter = load(data_args['resume_from_checkpoint'], model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    eval_iters = 100\n",
    "    for _ in range(eval_iters):\n",
    "        x, y = get_batch(valid_data, training_args['batch_size'], model_args['context_length'], device)\n",
    "        logits = model(x)\n",
    "        loss = loss_init(logits.view(-1, model_args['vocab_size']), y.view(-1))\n",
    "        valid_loss += loss.item()\n",
    "    model.train()\n",
    "    return valid_loss / eval_iters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Begin Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Training Loop ---\")\n",
    "t0 = time.time()\n",
    "for iter_num in range(start_iter, training_args['max_iters']):\n",
    "        \n",
    "    lr = compute_lr(\n",
    "        iter_num, \n",
    "        training_args['learning_rate'],\n",
    "        training_args['min_lr'],\n",
    "        training_args['warmup_steps'],\n",
    "        training_args['lr_decay_steps']\n",
    "    )\n",
    "        \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    inputs, targets = get_batch(train_data, training_args['batch_size'], model_args['context_length'], device)\n",
    "        \n",
    "    logits = model(inputs)\n",
    "    loss = loss_init(logits.view(-1, model_args['vocab_size']), targets.view(-1))\n",
    "        \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "        \n",
    "    gradient_cliping(model.parameters(), training_args['gradient_clip_val'])\n",
    "        \n",
    "    optimizer.step()\n",
    "        \n",
    "\n",
    "    if iter_num % 10 == 0:\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"Iter {iter_num}/{training_args['max_iters']}, Train Loss: {loss.item():.4f}, LR: {lr:.6f}, Time: {dt*1000:.2f}ms\")\n",
    "\n",
    "    if iter_num > 0 and iter_num % training_args['eval_interval'] == 0:\n",
    "        val_loss = evaluate()\n",
    "        print(f\"--- Eval at iter {iter_num}: Val Loss: {val_loss:.4f} ---\")\n",
    "            \n",
    "        checkpoint_path = os.path.join(data_args['checkpoint_dir'], f\"model_iter_{iter_num}.pt\")\n",
    "        save(model, optimizer, iter_num, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training Finished! ---\")\n",
    "final_checkpoint_path = os.path.join(data_args['checkpoint_dir'], \"model_final.pt\")\n",
    "save(model, optimizer, training_args['max_iters'], final_checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
